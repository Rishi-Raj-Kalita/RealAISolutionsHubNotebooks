{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "load_dotenv()\n",
    "\n",
    "def get_model(model: str = 'deepseek-r1:7b', provider: str = 'local'):\n",
    "    if (provider == 'local'):\n",
    "        from langchain_ollama import ChatOllama\n",
    "        llm = ChatOllama(model=model, temperature=0)\n",
    "        return llm\n",
    "    elif (provider == 'aws'):\n",
    "        from langchain_aws import ChatBedrockConverse\n",
    "        import boto3\n",
    "        access_key = os.getenv('ACCESS_KEY')\n",
    "        secret_key = os.getenv('SECRET_KEY')\n",
    "        bedrock_client = boto3.client('bedrock-runtime',\n",
    "                                      region_name='us-east-1',\n",
    "                                      aws_access_key_id=access_key,\n",
    "                                      aws_secret_access_key=secret_key)\n",
    "        llm = ChatBedrockConverse(client=bedrock_client,\n",
    "                                  model=model,\n",
    "                                  temperature=0)\n",
    "        return llm\n",
    "\n",
    "def get_embeddings(model:str='deepseek-r1:7b', provider:str='local'):\n",
    "    if(provider == 'local'):\n",
    "        from langchain_ollama import OllamaEmbeddings\n",
    "        embeddings=OllamaEmbeddings(model=model)\n",
    "        return embeddings\n",
    "    elif(provider == 'aws'):\n",
    "        from langchain_aws import BedrockEmbeddings\n",
    "        import boto3\n",
    "        access_key=os.getenv('ACCESS_KEY')\n",
    "        secret_key=os.getenv('SECRET_KEY')\n",
    "        bedrock_client=boto3.client('bedrock-runtime', region_name='us-east-1', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "        embeddings=BedrockEmbeddings(client=bedrock_client, model_id=model)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=get_model(model='anthropic.claude-3-5-sonnet-20241022-v2:0',provider='aws')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "\n",
    "def get_text_nodes(json_list: List[dict]):\n",
    "    text_nodes = []\n",
    "    for idx, page in enumerate(json_list):\n",
    "        text_node = TextNode(text=page[\"md\"], metadata={\"page\": page[\"page\"]})\n",
    "        text_nodes.append(text_node)\n",
    "    return text_nodes\n",
    "\n",
    "\n",
    "def save_jsonl(data_list, filename):\n",
    "    \"\"\"Save a list of dictionaries as JSON Lines.\"\"\"\n",
    "    with open(filename, \"w\") as file:\n",
    "        for item in data_list:\n",
    "            json.dump(item, file)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "\n",
    "def load_jsonl(filename):\n",
    "    \"\"\"Load a list of dictionaries from JSON Lines.\"\"\"\n",
    "    data_list = []\n",
    "    with open(filename, \"r\") as file:\n",
    "        for line in file:\n",
    "            data_list.append(json.loads(line))\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 4cc834ac-99fb-4939-a3b7-6504ed340b8d\n"
     ]
    }
   ],
   "source": [
    "from llama_cloud_services import LlamaParse\n",
    "\n",
    "parser = LlamaParse(\n",
    "    api_key=os.getenv(\"LAMA_PARSE_API_KEY\"),\n",
    "    result_type=\"markdown\",\n",
    "    use_vendor_multimodal_model=True,\n",
    "    vendor_multimodal_model_name=\"anthropic-sonnet-3.5\",\n",
    "    # invalidate_cache=True\n",
    ")\n",
    "json_objs = parser.get_json_result(\"./data/memes.pdf\")\n",
    "# json_objs = parser.get_json_result(\"./data/llama2-p33.pdf\")\n",
    "json_list = json_objs[0][\"pages\"]\n",
    "docs = get_text_nodes(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-04-25 00:29:22 [pypickle.pypickle] > INFO     > Pickle file saved: test.pkl\n"
     ]
    }
   ],
   "source": [
    "import pypickle\n",
    "\n",
    "import pypickle\n",
    "filepath = 'test.pkl'\n",
    "\n",
    "# Save\n",
    "status = pypickle.save(filepath, json_objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11-04-25 00:29:46 [pypickle.pypickle] > INFO     > Pickle file loaded: test.pkl\n"
     ]
    }
   ],
   "source": [
    "data = pypickle.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = data[0][\"pages\"]\n",
    "docs = get_text_nodes(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextNode(id_='981be608-094f-4eaa-969a-de53a29845d5', embedding=None, metadata={'page': 1}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='WIYOOREAQTSE\\n  ITS MONDAY\\n Memes can tell more than 1000 words.\\n      1', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_jsonl([d.dict() for d in docs], \"docs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nx/wvnbfw697mbf57dq4vcl2nbm0000gn/T/ipykernel_1428/218604384.py:4: PydanticDeprecatedSince20: The `parse_obj` method is deprecated; use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  docs = [Document.parse_obj(d) for d in docs_dicts]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "docs_dicts = load_jsonl(\"docs.jsonl\")\n",
    "docs = [Document.parse_obj(d) for d in docs_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page: 1\n",
      "\n",
      "WHEN YOU REALISE\n",
      "\n",
      "IT'S MONDAY\n",
      "\n",
      "Memes can tell more than 1000 words.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"./data/memes.pdf\"\n",
    "IMAGES_DOWNLOAD_PATH = \"./data/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 83664a32-4296-43e7-bfe9-9029fc10c362\n"
     ]
    }
   ],
   "source": [
    "parser = LlamaParse(\n",
    "    api_key=os.getenv(\"LAMA_PARSE_API_KEY\"),\n",
    "    result_type=\"markdown\",\n",
    ")\n",
    "\n",
    "json_objs = parser.get_json_result(FILE_NAME)\n",
    "json_list = json_objs[0][\"pages\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from llama_parse import LlamaParse\n",
    "from llama_index.core.schema import ImageDocument, TextNode\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Images for page 1: [{'name': 'img_p0_1.png', 'height': 1066, 'width': 1599, 'x': 168.139, 'y': 124.80138916800004, 'original_width': 1599, 'original_height': 1066, 'ocr': [{'x': 188, 'y': 3, 'w': 1270, 'h': 193, 'confidence': '0.2964225884886829', 'text': 'WIYOOREAQTSE'}, {'x': 391, 'y': 839, 'w': 858, 'h': 182, 'confidence': '0.9528591356940469', 'text': 'ITS MONDAY'}]}]\n"
     ]
    }
   ],
   "source": [
    "def get_text_nodes(json_list: List[dict]) -> List[TextNode]:\n",
    "    return [TextNode(text=page[\"text\"], metadata={\"page\": page[\"page\"]}) for page in json_list]\n",
    "\n",
    "text_nodes = get_text_nodes(json_list)\n",
    "\n",
    "def get_image_nodes(json_objs: List[dict], download_path: str) -> List[ImageDocument]:\n",
    "    image_dicts = parser.get_images(json_objs, download_path=download_path)\n",
    "    return [ImageDocument(image_path=image_dict[\"path\"]) for image_dict in image_dicts]\n",
    "\n",
    "image_documents = get_image_nodes(json_objs, IMAGES_DOWNLOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
